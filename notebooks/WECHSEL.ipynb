{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WECHSEL Tutorial\n",
    "\n",
    "In this tutorial, we will see how to use WECHSEL to transfer a model trained in English to German using Langsfer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from typing import Generator\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from langsfer.high_level import wechsel\n",
    "from langsfer.embeddings import TransformersEmbeddings, FastTextEmbeddings\n",
    "from langsfer.utils import download_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_MODEL_NAME = \"roberta-base\"\n",
    "DATASET_NAME = \"oscar-corpus/oscar\"\n",
    "DATASET_CONFIG_NAME = \"unshuffled_deduplicated_de\"\n",
    "DATASET_SIZE = 10000\n",
    "TRAIN_DATASET_SIZE = 8000\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "EVAL_STEPS = 4000\n",
    "MAX_TRAIN_STEPS = 16000\n",
    "SEED = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\n",
    "    DATASET_NAME,\n",
    "    DATASET_CONFIG_NAME,\n",
    "    split=\"train\",\n",
    "    streaming=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "dataset = dataset.shuffle(seed=SEED)\n",
    "dataset = dataset.take(DATASET_SIZE)\n",
    "train_dataset = dataset.take(TRAIN_DATASET_SIZE)\n",
    "val_dataset = dataset.skip(TRAIN_DATASET_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = list(val_dataset.skip(10).take(1))[0][\"text\"]\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings and Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_embeddings = TransformersEmbeddings.from_model_name_or_path(SOURCE_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = source_embeddings.tokenizer.tokenize(sample_text)\n",
    "print(f\"Number of tokens {len(tokens)}, tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train a new target tokenizer using the same configuration as the source tokenizer using the training dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterator(\n",
    "    dataset: datasets.Dataset, batch_size: int = 1000\n",
    ") -> Generator[str, None, None]:\n",
    "    for batch in dataset.iter(batch_size=batch_size):\n",
    "        yield batch[\"text\"]\n",
    "\n",
    "\n",
    "target_tokenizer = source_embeddings.tokenizer.train_new_from_iterator(\n",
    "    batch_iterator(train_dataset), vocab_size=len(source_embeddings.tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = target_tokenizer.tokenize(sample_text)\n",
    "print(f\"Number of tokens {len(tokens)}, tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load pre-trained fasttext embeddings to use as auxiliary embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_auxiliary_embeddings = FastTextEmbeddings.from_model_name_or_path(\"en\")\n",
    "source_auxiliary_embeddings = FastTextEmbeddings.from_model_name_or_path(\"de\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we download a bilinigual dictionary for English and German in order to be able to align the auxiliary embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilingual_dictionary_file = download_file(\n",
    "    \"https://raw.githubusercontent.com/CPJKU/wechsel/main/dicts/data/german.txt\",\n",
    "    \"german.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we open the file and read the first few lines, we can see that it maps English words to their German equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with bilingual_dictionary_file.open() as f:\n",
    "    dictionary_lines = [dict([f.readline().strip().split(\"\\t\")]) for _ in range(10)]\n",
    "\n",
    "dictionary_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally, instantiate the embedding initializer for WECHSEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_initializer = wechsel(\n",
    "    source_embeddings=source_embeddings,\n",
    "    target_tokenizer=target_tokenizer,\n",
    "    target_auxiliary_embeddings=target_auxiliary_embeddings,\n",
    "    source_auxiliary_embeddings=source_auxiliary_embeddings,\n",
    "    bilingual_dictionary_file=bilingual_dictionary_file,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then initialize the target embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_embeddings = embedding_initializer.initialize(seed=16, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = AutoModelForCausalLM.from_pretrained(SOURCE_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model.get_input_embeddings().weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model.get_input_embeddings().weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize its embedding layer\n",
    "target_model.resize_token_embeddings(len(target_tokenizer))\n",
    "# Replace the source embeddings matrix with the target embeddings matrix\n",
    "target_model.get_input_embeddings().weight.data = torch.as_tensor(\n",
    "    target_embeddings.embeddings_matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model.get_input_embeddings().weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model.get_input_embeddings().weight.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of comprison, we additionally initialize a similar model but with a random initialization for the embeddings layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model_from_scratch = AutoModelForCausalLM.from_pretrained(SOURCE_MODEL_NAME)\n",
    "target_model.get_input_embeddings().weight.data = torch.normal(\n",
    "    torch.mean(source_embeddings.embeddings_matrix, axis=0),\n",
    "    torch.std(source_embeddings.embeddings_matrix, axis=0),\n",
    "    (\n",
    "        len(target_tokenizer),\n",
    "        source_embeddings.embeddings_matrix.shape[1],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "    lambda x: target_tokenizer(x[\"text\"], truncation=True),\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names,\n",
    ")\n",
    "train_dataset = train_dataset.with_format(\"torch\")\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "    lambda x: target_tokenizer(x[\"text\"], truncation=True),\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names,\n",
    ")\n",
    "val_dataset = val_dataset.with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=target_tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"from_scratch\",\n",
    "    eval_strategy=\"steps\",\n",
    "    report_to=\"tensorboard\",\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    max_steps=MAX_TRAIN_STEPS,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    bf16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=target_model_from_scratch,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=target_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loss = trainer.evaluate()[\"eval_loss\"]\n",
    "print(f\"Evaluation loss before training: {eval_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loss = trainer.evaluate()[\"eval_loss\"]\n",
    "print(f\"Evaluation loss after training: {eval_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input_ids = target_tokenizer(sample_text)[\"input_ids\"]\n",
    "shortened_input_ids = sample_input_ids[: len(sample_input_ids) // 3]\n",
    "\n",
    "generated_token_ids = (\n",
    "    trainer.model.generate(\n",
    "        torch.as_tensor(shortened_input_ids).reshape(1, -1).to(trainer.model.device),\n",
    "        max_length=300,\n",
    "    )\n",
    "    .detach()\n",
    "    .cpu()\n",
    "    .numpy()\n",
    "    .reshape(-1)\n",
    ")\n",
    "generated_token_ids = target_tokenizer.decode(\n",
    "    generated_token_ids, add_special_tokens=False\n",
    ")\n",
    "print(\"Original Text:\")\n",
    "print(sample_text)\n",
    "print(\"---\")\n",
    "print(\"Generated Text:\")\n",
    "print(generated_token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model with initialized embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"initialized_embedding\",\n",
    "    eval_strategy=\"steps\",\n",
    "    report_to=\"tensorboard\",\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    max_steps=MAX_TRAIN_STEPS,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    bf16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=target_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=target_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the model before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loss = trainer.evaluate()[\"eval_loss\"]\n",
    "print(f\"Evaluation loss before training: {eval_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally evaluate the model after the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loss = trainer.evaluate()[\"eval_loss\"]\n",
    "print(f\"Evaluation loss after training: {eval_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input_ids = target_tokenizer(sample_text)[\"input_ids\"]\n",
    "shortened_input_ids = sample_input_ids[: len(sample_input_ids) // 3]\n",
    "\n",
    "generated_token_ids = (\n",
    "    trainer.model.generate(\n",
    "        torch.as_tensor(shortened_input_ids).reshape(1, -1).to(trainer.model.device),\n",
    "        max_length=300,\n",
    "    )\n",
    "    .detach()\n",
    "    .cpu()\n",
    "    .numpy()\n",
    "    .reshape(-1)\n",
    ")\n",
    "generated_token_ids = target_tokenizer.decode(\n",
    "    generated_token_ids, add_special_tokens=False\n",
    ")\n",
    "print(\"Original Text:\")\n",
    "print(sample_text)\n",
    "print(\"---\")\n",
    "print(\"Generated Text:\")\n",
    "print(generated_token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langsfer-I0oHYpHZ-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
